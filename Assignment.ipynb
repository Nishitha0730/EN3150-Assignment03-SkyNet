{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0338b2ff",
   "metadata": {},
   "source": [
    "# EN3150 Assignment03- Simple convolutional neural network to perform classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2bb993",
   "metadata": {},
   "source": [
    "# Custom Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e55466",
   "metadata": {},
   "source": [
    "## 1. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3dd6330b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required libraries\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, random_split, ConcatDataset\n",
    "from torchvision import datasets, transforms\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38f6a77",
   "metadata": {},
   "source": [
    "### 1.1 Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66bd2624",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(), # Convert images to PyTorch tensors and scale to [0, 1]\n",
    "    transforms.Normalize((0.5,), (0.5,)) # Normalize to mean=0.5, std=0.5\n",
    "])\n",
    "\n",
    "# Load the MNIST dataset\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Combine into a single full dataset\n",
    "full_dataset = ConcatDataset([train_dataset, test_dataset])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbb8259",
   "metadata": {},
   "source": [
    "### 1.2 Data spliting and Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8af5e4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into 70/15/15\n",
    "train_size = int(0.7 * len(full_dataset))\n",
    "val_size = int(0.15 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size - val_size\n",
    "\n",
    "# Split the dataset into train, validation, and test sets\n",
    "train_data, val_data, test_data = random_split(full_dataset, [train_size, val_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e3b5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data into DataLoader for batching\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=64, shuffle=False)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# CNN Model Implementation",
   "id": "f1b5c3cd085ae6a5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        # 1st Conv Block\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)  # Batch normalization\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        # 2nd Conv Block\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)  # Batch normalization\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        # 3rd Conv Block\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(128 * 3 * 3, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc3 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.pool3(x)\n",
    "\n",
    "        # Classifier\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ],
   "id": "7738644c5a07f00d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
